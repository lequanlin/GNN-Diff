name: node_classification_large
model_name: sage

data:
  data_root: data\reddit
  dataset: reddit
  num_workers: 1

# model for data
model:
  _target_: models.sage.SAGE_2_conv
  in_channels: 602
  out_channels: 41
  num_hid: 256
  dropout: 0.1

# optimizer1 is for full model training, optimizer 2 is for further tuning of sample layers
optimizer1:
  _target_: torch.optim.Adam
  lr: 0.0005
  weight_decay: 0.0005

optimizer2:
  _target_: torch.optim.Adam
  lr: 0.0005
  weight_decay: 0.0005


# lr scheduler for training task optimizer
lr_scheduler:
  _target_: torch.optim.lr_scheduler.MultiStepLR
  milestones: [150, 170]
  gamma: 0.2

# epoch is #epoch for full model training, save_num_model is for sampling
epoch: 200
save_num_model: 100

# name of target layers
train_layer: ['conv2.lin_l.weight', 'conv2.lin_l.bias', 'conv2.lin_r.weight']

# parameter data root
param:
  data_root: param_data/sage/reddit/data.pt
  k: 50
  num_workers: 4
